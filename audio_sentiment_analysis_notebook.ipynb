{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1i8Q9MzN-AaO4IFKJbW30XirRm75SnguB",
      "authorship_tag": "ABX9TyNJ2FVsFi6Mpa1TwYxo+inz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rp-bot/audio-sentiment-analysis/blob/main/audio_sentiment_analysis_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dDIGY7ojvj0r"
      },
      "outputs": [],
      "source": [
        "home_dir = \"/content/drive/MyDrive/Colab Notebooks/audio_sentiment_analysis\"\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Neural Network Stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the dataset"
      ],
      "metadata": {
        "id": "Tb-yEE4xdndK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory containing your audio files\n",
        "audio_dir = os.path.join(home_dir, 'data')\n",
        "output_dir = os.path.join(home_dir, 'output_samples')\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "TL64E2U1xSrH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target sampling rate and duration for each sample\n",
        "target_sr = 16000  # Target sampling rate of 16 kHz\n",
        "sample_duration = 2.0  # Duration of each sample in seconds"
      ],
      "metadata": {
        "id": "tnAd6T_lb2_V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cqt_spectrograms(file_path):\n",
        "    # Load audio file\n",
        "    y, sr = librosa.load(file_path, sr=target_sr)\n",
        "\n",
        "    # Calculate number of samples per segment\n",
        "    samples_per_segment = int(target_sr * sample_duration)\n",
        "    num_segments = int(len(y) / samples_per_segment)\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        start_sample = i * samples_per_segment\n",
        "        end_sample = start_sample + samples_per_segment\n",
        "\n",
        "        # Extract segment\n",
        "        segment = y[start_sample:end_sample]\n",
        "\n",
        "        # Generate CQT spectrogram\n",
        "        cqt_spectrogram = librosa.cqt(segment, sr=sr, hop_length=512)\n",
        "        cqt_db = librosa.amplitude_to_db(np.abs(cqt_spectrogram), ref=np.max)\n",
        "\n",
        "        # Save or display spectrogram\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        librosa.display.specshow(cqt_db, sr=sr, x_axis='time', y_axis='cqt_note')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'CQT Spectrogram - Segment {i}')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot as image file\n",
        "        output_filename = f\"{os.path.splitext(os.path.basename(file_path))[0]}_segment_{i}.png\"\n",
        "        plt.savefig(os.path.join(output_dir, output_filename))\n",
        "        plt.close()\n",
        "\n",
        "# Process each audio file in the directory\n",
        "for filename in os.listdir(audio_dir):\n",
        "    if filename.endswith('.mp3') or filename.endswith('.wav'):\n",
        "        file_path = os.path.join(audio_dir, filename)\n",
        "        generate_cqt_spectrograms(file_path)\n",
        "\n",
        "print(\"Spectrogram generation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD_6ewxkcAtN",
        "outputId": "dc992967-043e-44c9-b4c5-cca59ba1e92b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spectrogram generation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating  Spectrogram"
      ],
      "metadata": {
        "id": "Sd0sFK9Xdrgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def generate_cqt_spectrograms(audio_path, output_dir, sample_rate=16000, segment_duration=2):\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
        "\n",
        "    # Calculate number of samples per segment\n",
        "    samples_per_segment = int(sr * segment_duration)\n",
        "\n",
        "    # Split audio into segments and generate CQT spectrogram for each\n",
        "    num_segments = len(y) // samples_per_segment\n",
        "    for i in range(num_segments):\n",
        "        start_sample = i * samples_per_segment\n",
        "        end_sample = start_sample + samples_per_segment\n",
        "\n",
        "        # Extract segment\n",
        "        segment = y[start_sample:end_sample]\n",
        "\n",
        "        # Generate CQT spectrogram\n",
        "        cqt = librosa.cqt(segment, sr=sr)\n",
        "        cqt_db = librosa.amplitude_to_db(np.abs(cqt), ref=np.max)\n",
        "\n",
        "        # Plot and save the spectrogram\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        librosa.display.specshow(cqt_db, sr=sr, x_axis='time', y_axis='cqt_note')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title('CQT Spectrogram')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot as an image file\n",
        "        output_file = os.path.join(output_dir, f'segment_{i}.png')\n",
        "        plt.savefig(output_file)\n",
        "        plt.close()\n",
        "\n",
        "# Example usage\n",
        "audio_file_path = 'path/to/your/audio/file.mp3'\n",
        "output_directory = 'path/to/save/spectrograms'\n",
        "generate_cqt_spectrograms(audio_file_path, output_directory)"
      ],
      "metadata": {
        "id": "QDASdoLFeytF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-4 Model Mentioned in the paper about emotion classification in movies\n",
        "# (Lucia-Mulas et al. 2023)\n",
        "class CNN4Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN4Model, self).__init__()\n",
        "\n",
        "        # Define convolutional layers with Batch Normalization and MaxPooling\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Additional convolutional layers for more depth\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(256 * 10 * 10, 512)  # Adjust dimensions based on input size\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(512, 4)  # Output layer for 4 emotions\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN4Model()\n",
        "\n",
        "# Print model architecture\n",
        "print(model.parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNMgUYRqcDaN",
        "outputId": "fb13b308-fc36-4446-f2cd-e05f9be298f2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.parameters of CNN4Model(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=25600, out_features=512, bias=True)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
            ")>\n"
          ]
        }
      ]
    }
  ]
}